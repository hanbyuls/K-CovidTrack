# Refit the logistic regression using just Lag1 and Lag2
glm.fits <- glm (Direction ~ Lag1 + Lag2 , data = Smarket ,
family = binomial , subset = train)
glm.probs <- predict (glm.fits , Smarket.2005,
type = "response")
glm.pred <- rep ("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table (glm.pred , Direction.2005)
mean (glm.pred == Direction .2005)
mean (glm.pred == Direction.2005)
106 / (106 + 76)
predict (glm.fits,
newdata =
data.frame (Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response"
)
######### Linear Discriminant Analysis #########
library (MASS)
lda.fit <- lda (Direction ~ Lag1 + Lag2 , data = Smarket ,
subset = train)
lda.fit
lda.pred <- predict (lda.fit , Smarket.2005)
names(lda.pred)
lda.class <- lda.pred$class
table (lda.class, Direction .2005)
table (lda.class, Direction.2005)
mean (lda.class == Direction.2005)
lda.pred$class
View(data.lda.values)
Species=Smarket.2005$Direction
Smarket.2005$Direction
lda.class$x[,2]
lda.class
lda.class <- lda.pred$class
lda.class
data.lda.values$x[,1]
data.lda.values$x[,2]
plot (lda.fit)
# Fit the model using only the observations before 2005.
lda.fit <- lda(Direction ~ Lag1 + Lag2 , data = Smarket ,
subset = train)
lda.fit
# produces plots of the linear discriminant
plot (lda.fit)
sum (lda.pred$posterior[, 1] < .5)
# Applying a 50 % threshold to the posterior probabilities allows us to recreate
# the predictions contained in lda.pred$class
sum (lda.pred$posterior[, 1] >= .5)
sum (lda.pred$posterior[, 1] < .5)
# Set different posterior probability threshold
sum(lda.pred$posterior[, 1] > .9)
qda.fit
# Fit the model using only the observations before 2005.
qda.fit <- qda (Direction ~ Lag1 + Lag2 , data = Smarket ,
subset = train)
qda.fit
# The predict() function works in exactly the same fashion as for LDA.
qda.class <- predict (qda.fit , Smarket.2005)$class
table (qda.class , Direction.2005)
mean (qda.class == Direction.2005)
######### Naive Bayes #########
# Use naiveBayes() function from e1071 library
library (e1071)
nb.fit <- naiveBayes (Direction ~ Lag1 + Lag2 , data = Smarket ,
subset = train)
nb.fit
# predict function
nb.class <- predict (nb.fit , Smarket.2005)
table (nb.class , Direction.2005)
mean (nb.class == Direction.2005)
# Predict() function can also generate estimates of the probability
# that each observation belongs to a particular class.
nb.preds <- predict (nb.fit , Smarket.2005, type = " raw ")
# Predict() function can also generate estimates of the probability
# that each observation belongs to a particular class.
nb.preds <- predict (nb.fit , Smarket.2005, type = "raw")
nb.preds[1:5, ]
## create a dataframe that has all the info we need to draw a graph
plot.data <- data.frame(X=data.lda.values$x[,1], Y=data.lda.values$x[,2], Species=my.data$Species)
head(plot.data)
## draw a graph using ggplot2
p <- ggplot(data=plot.data, aes(x=X, y=Y)) +
geom_point(aes(color=Species)) +
theme_bw()
p
# Q1.
install.packages("MPV")
library(MPV)
library(dplyr)
library(ggplot2)
library(faraway)
library(lmtest) # for Durbin-Watson test
library(ggcorrplot) # for correlation plot
library(olsrr)
library(caret)
library(mlbench) # for dataset BostonHousing
library(glmnet)
suppressPackageStartupMessages({
library(dplyr)
library(ggplot2)
library(faraway)
library(lmtest) # for Durbin-Watson test
library(ggcorrplot) # for correlation plot
library(olsrr)
library(caret)
library(mlbench) # for dataset BostonHousing
library(glmnet)
})
# Q1.
library(dplyr)
library(ggplot2)
library(faraway)
library(lmtest) # for Durbin-Watson test
library(ggcorrplot) # for correlation plot
library(olsrr)
library(caret)
library(mlbench) # for dataset BostonHousing
# Q1.
install.packages("MPV")
library(MPV)
library(dplyr)
library(ggplot2)
library(faraway)
library(lmtest) # for Durbin-Watson test
library(ggcorrplot) # for correlation plot
library(olsrr)
library(caret)
library(mlbench) # for dataset BostonHousing
library(glmnet)
library(dplyr)
library(ggplot2)
library(faraway)
library(lmtest) # for Durbin-Watson test
library(ggcorrplot) # for correlation plot
library(olsrr)
library(caret)
library(mlbench) # for dataset BostonHousing
library(glmnet)
install.packages('mlbench')
library(mlbench) # for dataset BostonHousing
library(glmnet)
data = as.data.frame(MPV::table.b1)
data
View(data)
# fit_multiple_linear_regression
fit_multiple_linear_regression <- lm(y ~ ., data = data)
## 1-a)
# Forward selection regression (alpha = 0.1)
# i) with p-val
fit_multiple_linear_regression_forward_p <-
olsrr::ols_step_forward_p(fit_multiple_linear_regression, penter = 0.1, detail = TRUE)
plot(fit_multiple_linear_regression_forward_p)
fit_multiple_linear_regression_forward_p
summary(stats::update(fit_multiple_linear_regression, ~ . - Nearest))
# ii) with aic
fit_multiple_linear_regression_forward_aic <-
olsrr::ols_step_forward_aic(fit_multiple_linear_regression, detail = TRUE)
plot(fit_multiple_linear_regression_forward_aic)
fit_multiple_linear_regression_forward_aic
summary(stats::update(fit_multiple_linear_regression, ~ . - Nearest - Area - Scruz))
stats::AIC(stats::update(fit_multiple_linear_regression, ~ . - Nearest - Area - Scruz))
## 1-b)
# Backward elimination regression (alpha = 0.1)
# i) with p-val
fit_multiple_linear_regression_backward_p <-
olsrr::ols_step_backward_p(fit_multiple_linear_regression, prem = 0.1, progress = TRUE)
# plot(fit_multiple_linear_regression_backward_p)
fit_multiple_linear_regression_backward_p
summary(stats::update(fit_multiple_linear_regression, ~ . - Nearest))
# ii) with aic
fit_multiple_linear_regression_backward_aic <-
olsrr::ols_step_backward_aic(fit_multiple_linear_regression, detail = TRUE)
plot(fit_multiple_linear_regression_backward_aic)
fit_multiple_linear_regression_backward_aic
stats::AIC(stats::update(fit_multiple_linear_regression, ~ . - Nearest - Area - Scruz))
## 1-c)
# Stepwise regression
fit_multiple_linear_regression_stepwise_p <-
olsrr::ols_step_both_p(fit_multiple_linear_regression, pent = 0.05, prem = 0.1, progress = TRUE)
plot(fit_multiple_linear_regression_stepwise_p)
fit_multiple_linear_regression_stepwise_p
summary(stats::update(fit_multiple_linear_regression, ~ . - Nearest - Area - Scruz))
fit_multiple_linear_regression_stepwise_aic <-
olsrr::ols_step_both_aic(fit_multiple_linear_regression, detail = TRUE)
plot(fit_multiple_linear_regression_stepwise_aic)
fit_multiple_linear_regression_stepwise_aic
stats::AIC(stats::update(fit_multiple_linear_regression, ~ . - Nearest - Area - Scruz))
library(glmnet)
library(ggplot2)
temp <- c(53,56,57,63,66,67,67,67,68,69,70,70,70,70,72,73,75,75,76,76,78,79,80,81)
failure <- c(1,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0)
failure_df <- data.frame(temp)
failure_df$failure <- failure
# Fit logistic regression model
model <- glm(failure ~ temp, data = failure_df, family = binomial)
summary(model)
# Calculate the deviance residuals
dev_res <- residuals(model, type = "deviance")
# Add deviance residuals to the failure_df dataframe
failure_df$deviance_residuals <- dev_res
# Calculate logistic regression predictions
failure_df$predicted_probs <- predict(model, newdata = failure_df, type = "response")
# Plot logistic regression S-curve line
ggplot(failure_df, aes(x = temp, y = failure)) +
geom_point(colour = "blue") +
geom_line(data = failure_df, aes(x = temp, y = predicted_probs), colour = "red", size = 1) +
labs(x = "Temperature", y = "Failure") +
theme_minimal()
newdata <- data.frame(temp = 50)  # New data point with temperature 5
predicted_prob <- predict(model, newdata = newdata, type = "response")
failure_prob <- predicted_prob[1]  # Extract the failure probability
# Print the estimated failure probability
cat("Estimated failure probability at temperature 50:", failure_prob, "\n")
# Extract coefficient
coef <- coef(model)["temp"]
# Exponentiate
odds_ratio <- exp(coef)
odds_ratio
# Fit logistic regression model with quadratic term
model <- glm(failure ~ temp + I(temp^2), data = failure_df, family = binomial)
summary(model)
# Calculate logistic regression predictions
failure_df$predicted_probs <- predict(model, newdata = failure_df, type = "response")
# Plot logistic regression S-curve line
ggplot(failure_df, aes(x = temp, y = failure)) +
geom_point(colour = "blue") +
geom_line(data = failure_df, aes(x = temp, y = predicted_probs), colour = "red", size = 1) +
labs(x = "Temperature", y = "Failure") +
theme_minimal()
# Convert predicted probabilities to binary predictions
threshold <- 0.5
predicted_classes <- ifelse(failure_df$predicted_probs >= threshold, 1, 0)
# Create confusion matrix
confusion_matrix <- table(Actual = failure_df$failure, Predicted = predicted_classes)
print(confusion_matrix)
rm(list = ls()); gc()
options(conflicts.policy = list(warn = FALSE))
shhh <- suppressPackageStartupMessages # It's a library, so shhh!
shhh(library(av))
shhh(library(bslib))
shhh(library(cptcity))
shhh(library(countrycode))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lattice))
shhh(library(latticeExtra))
shhh(library(maps))
shhh(library(markdown))
shhh(library(purrr))
options("rgdal_show_exportToProj4_warnings"="none")
shhh(library(raster, warn.conflicts = FALSE)) # classes and functions for raster data
shhh(library(rasterVis))
shhh(library(readxl))
shhh(library(writexl))
shhh(library(rgdal, warn.conflicts = FALSE))
shhh(library(shiny))
shhh(library(shinyalert))
shhh(library(shinyvalidate))
shhh(library(shinyjs))
shhh(library(shinyhelper))
shhh(library(shinyWidgets))
shhh(library(sp))
shhh(library(sf))     # classes and functions for vector data
shhh(library(stringr))
shhh(library(terra, warn.conflicts=FALSE))  # suppressWarnings(suppressMessages(library(terra)))
shhh(library(tinytex))
shhh(library(shinythemes))
shhh(library(readxl))
shhh(library(dplyr))
shhh(library(leaflet))
shhh(library(leaflet.extras))
shhh(library(htmltools))
shhh(library(shinyBS))
shhh(library(plotly))
shhh(library(shinydashboard))
shhh(library(DT))
shhh(library(zoo))
shhh((library(lubridate)))
shhh((library(rsconnect)))
options(conflicts.policy = list(warn = FALSE))
shhh <- suppressPackageStartupMessages # It's a library, so shhh!
shhh(library(av))
shhh(library(bslib))
shhh(library(cptcity))
shhh(library(countrycode))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lattice))
shhh(library(latticeExtra))
shhh(library(maps))
shhh(library(markdown))
shhh(library(purrr))
options("rgdal_show_exportToProj4_warnings"="none")
shhh(library(raster, warn.conflicts = FALSE)) # classes and functions for raster data
shhh(library(rasterVis))
shhh(library(readxl))
shhh(library(writexl))
shhh(library(rgdal, warn.conflicts = FALSE))
shhh(library(shiny))
shhh(library(shinyalert))
shhh(library(shinyvalidate))
shhh(library(shinyjs))
shhh(library(shinyhelper))
shhh(library(shinyWidgets))
shhh(library(sp))
shhh(library(sf))     # classes and functions for vector data
shhh(library(stringr))
shhh(library(terra, warn.conflicts=FALSE))  # suppressWarnings(suppressMessages(library(terra)))
shhh(library(tinytex))
shhh(library(shinythemes))
shhh(library(readxl))
shhh(library(dplyr))
shhh(library(leaflet))
shhh(library(leaflet.extras))
shhh(library(htmltools))
shhh(library(shinyBS))
shhh(library(plotly))
shhh(library(shinydashboard))
shhh(library(DT))
shhh(library(zoo))
shhh((library(lubridate)))
shhh((library(rsconnect)))
rsconnect::setAccountInfo(name='hbsong', token='B3B8FD441A72C9B327FBBBBDF8012B7C', secret='pUawtlS+Rpdh5qCeUwke0xfSWCJCyR0qNFzem0/n')
rsconnect::deployApp('C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim')
# select models
models <- c("SEIQRDV3P", "ARIMA","GAM", "tsglm", "lightGBM", "BILSTM")
model_names <- c("Extended SEIR", "ARIMA","GAM", "Time Series Poisson", "LightGBM", "Bi-LSTM")
n_models <- length(models)
data_types <- c("new_cases", "new_deaths", "icu_patients")
data_type_names <- c("Daily Confirmed Cases", "Daily Confirmed Deaths", "Daily ICU Patients")
n_data_types <- length(data_types)
m <- models[3] # Here1
m_n <- model_names[3]
# Here2 This part is important!!!!
selected_models <- list("new_cases_raw" = "pred_NULL", "new_cases_smoothed" = "X3rd_omicron_ba5",
"new_deaths_raw" = "X3rd_omicron_ba5", "new_deaths_smoothed" = "X3rd_omicron_ba5",
"icu_patients_raw" = "X3rd_omicron_ba5", "icu_patients_smoothed" = "X3rd_omicron_ba5")
i_c <- "KOR"
d_i <- 1
#d_i <- 1
d <- data_types[d_i]
d_n <- data_type_names[d_i]
d
d_n
r <- "raw"
paste0(input_path, i_c, "_",d,"_", m,"_",r,".csv")
# Change to your own directory
data_path <- paste0("C:/test/COVID-19/Data/") # Path to the original owid dataset
input_path <- paste0("C:/test/COVID-19/Result/220923_total/data/") # Path to the best fitted model result & block IDs
save_path <- paste0("C:/test/COVID-19/Result/230403_bootstrap_fixed_idx/bootstrap_data/") # Path to save the bootstrap datastet
input_path, i_c, "_",d,"_", m,"_",r,".csv")
paste0(input_path, i_c, "_",d,"_", m,"_",r,".csv")
rm(list = ls()); gc()
options(conflicts.policy = list(warn = FALSE))
shhh <- suppressPackageStartupMessages # It's a library, so shhh!
shhh(library(av))
shhh(library(bslib))
shhh(library(cptcity))
shhh(library(countrycode))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lattice))
shhh(library(latticeExtra))
shhh(library(maps))
shhh(library(markdown))
shhh(library(purrr))
options("rgdal_show_exportToProj4_warnings"="none")
shhh(library(raster, warn.conflicts = FALSE)) # classes and functions for raster data
shhh(library(rasterVis))
shhh(library(readxl))
shhh(library(writexl))
shhh(library(rgdal, warn.conflicts = FALSE))
shhh(library(shiny))
shhh(library(shinyalert))
shhh(library(shinyvalidate))
shhh(library(shinyjs))
shhh(library(shinyhelper))
shhh(library(shinyWidgets))
shhh(library(sp))
shhh(library(sf))     # classes and functions for vector data
shhh(library(stringr))
shhh(library(terra, warn.conflicts=FALSE))  # suppressWarnings(suppressMessages(library(terra)))
shhh(library(tinytex))
shhh(library(shinythemes))
shhh(library(readxl))
shhh(library(dplyr))
shhh(library(leaflet))
shhh(library(leaflet.extras))
shhh(library(htmltools))
shhh(library(shinyBS))
shhh(library(plotly))
shhh(library(shinydashboard))
shhh(library(DT))
shhh(library(zoo))
shhh((library(lubridate)))
shhh((library(rsconnect)))
# rsconnect::setAccountInfo(name='hbsong', token='B3B8FD441A72C9B327FBBBBDF8012B7C', secret='pUawtlS+Rpdh5qCeUwke0xfSWCJCyR0qNFzem0/n')
# rsconnect::deployApp('C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim')
############### Load data ###############
# setwd("C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim")
population <- read_excel("misc/population.xlsx", 1)
epiparms <- read_excel("misc/epiparms.xlsx", 1)
# Read data from CSV file
data <- read.csv("Korea viz/data/covid_data.csv")
data_korea <- read.csv("Korea viz/data/data_korea.csv")
region_population <- read_excel("Korea viz/data/region_population.xlsx")
population <- read_excel("misc/population.xlsx", 1)
epiparms <- read_excel("misc/epiparms.xlsx", 1)
region_df <- read.csv("Korea viz/data/region_data.csv")
full_region_df <- read.csv("Korea viz/data/region_data_05_31.csv")
# Read data for map
worldcountry = geojsonio::geojson_read("Korea viz/data/input_data/50m.geojson", what = "sp")
cv_cases = read.csv("Korea viz/data/input_data/coronavirus.csv")
countries = read.csv("Korea viz/data/input_data//countries_codes_and_coordinates.csv")
south_korea <- sf::st_read("Korea viz/data/south-korea-with-regions_1516.geojson")
region_match <- read.csv("Korea viz/data/region_match.csv")
rm(list = ls()); gc()
options(conflicts.policy = list(warn = FALSE))
shhh <- suppressPackageStartupMessages # It's a library, so shhh!
shhh(library(av))
shhh(library(bslib))
shhh(library(cptcity))
shhh(library(countrycode))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lattice))
shhh(library(latticeExtra))
shhh(library(maps))
shhh(library(markdown))
shhh(library(purrr))
options("rgdal_show_exportToProj4_warnings"="none")
shhh(library(raster, warn.conflicts = FALSE)) # classes and functions for raster data
shhh(library(rasterVis))
shhh(library(readxl))
shhh(library(writexl))
shhh(library(rgdal, warn.conflicts = FALSE))
shhh(library(shiny))
shhh(library(shinyalert))
shhh(library(shinyvalidate))
shhh(library(shinyjs))
shhh(library(shinyhelper))
shhh(library(shinyWidgets))
shhh(library(sp))
shhh(library(sf))     # classes and functions for vector data
shhh(library(stringr))
shhh(library(terra, warn.conflicts=FALSE))  # suppressWarnings(suppressMessages(library(terra)))
shhh(library(tinytex))
shhh(library(shinythemes))
shhh(library(readxl))
shhh(library(dplyr))
shhh(library(leaflet))
shhh(library(leaflet.extras))
shhh(library(htmltools))
shhh(library(shinyBS))
shhh(library(plotly))
shhh(library(shinydashboard))
shhh(library(DT))
shhh(library(zoo))
shhh((library(lubridate)))
shhh((library(rsconnect)))
population <- read_excel("misc/population.xlsx", 1)
setwd("C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim")
population <- read_excel("misc/population.xlsx", 1)
epiparms <- read_excel("misc/epiparms.xlsx", 1)
# Read data from CSV file
data <- read.csv("Korea viz/data/covid_data.csv")
data_korea <- read.csv("Korea viz/data/data_korea.csv")
region_population <- read_excel("Korea viz/data/region_population.xlsx")
population <- read_excel("misc/population.xlsx", 1)
epiparms <- read_excel("misc/epiparms.xlsx", 1)
region_df <- read.csv("Korea viz/data/region_data.csv")
full_region_df <- read.csv("Korea viz/data/region_data_05_31.csv")
# Read data for map
worldcountry = geojsonio::geojson_read("Korea viz/data/input_data/50m.geojson", what = "sp")
cv_cases = read.csv("Korea viz/data/input_data/coronavirus.csv")
countries = read.csv("Korea viz/data/input_data//countries_codes_and_coordinates.csv")
south_korea <- sf::st_read("Korea viz/data/south-korea-with-regions_1516.geojson")
region_match <- read.csv("Korea viz/data/region_match.csv")
setwd("C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim/Korea viz/data/")
# select a month from region population
colnames(region_population)[1] <- 'region'
colnames(region_population)[2] <- 'population'
region_population <- subset(region_population, !(region %in% c("행정구역(시군구)별", "전국")))
region_population <- region_population[,1:2]
region_population$region <- region_match$eng[match(region_population$region, region_match$kor)]
# Remove total and lazaretto in region_df
region_df <- subset(region_df, !(region %in% c("Total", "Lazaretto")))
full_region_df <- subset(full_region_df, !(region %in% c("Total", "Lazaretto")))
# Find indices of zero values in cum_deaths
zero_indices <- which(full_region_df$cum_deaths == 0)
# Get the index of the start time point from where you want to interpolate
start_time_point <- 1000
# Filter out zero_indices that are greater than or equal to start_time_point
zero_indices_to_interpolate <- zero_indices[zero_indices >= start_time_point]
# Loop through the zero indices and replace with interpolated values
for (i in zero_indices_to_interpolate) {
prev_day <- max(which(full_region_df$cum_deaths[1:i] > 0))
next_day <- min(which(full_region_df$cum_deaths[i:length(full_region_df$cum_deaths)] > 0))
interpolated_value <- approx(x = c(prev_day, next_day), y = full_region_df$cum_deaths[c(prev_day, next_day)], xout = i)$y
full_region_df$cum_deaths[i] <- interpolated_value
}
View(full_region_df)
runApp('C:/Users/ron18/Desktop/2022-1/bibs/covid19/spatialEpisim')
zero_indices_to_interpolate
zero_indices_cum_case <- which(full_region_df$cum_cases == 0)
zero_indices_cum_case
# Find indices of zero values in daily_cases
zero_indices_cum_case <- which(full_region_df$daily_cases == 0)
zero_indices_cum_case
# Find indices of zero values in daily_cases
zero_indices_cum_case <- which(full_region_df$daily_cases == 0)
zero_indices_cum_case
